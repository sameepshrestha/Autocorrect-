{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autocorrect_max_probability.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjD7UrEwn2z7VmwA2a19Lu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameepshrestha/Autocorrect-/blob/main/Autocorrect_max_probability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1oy3tQbD95n"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import nltk \n",
        "import matplotlib.pyplot as plt\n",
        "import re \n",
        "from collections import Counter"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvMZVmXP9W0O"
      },
      "source": [
        "#to break the paragraph into each sentence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g4kUB-9EIk5"
      },
      "source": [
        "class Sentinizer():\n",
        "    def __init__(self,input_text,splitter=['.','?','!',':'],delimiter_token='<split>'):\n",
        "        # self.sentences = []\n",
        "        self.raw = input_text\n",
        "        self._split_characters = splitter\n",
        "        self._delimiter_token = delimiter_token\n",
        "        self._index = 0\n",
        "        self._sentencize()\n",
        "    def _sentencize(self):\n",
        "        work_sentence = self.raw\n",
        "        for characters in self._split_characters:\n",
        "            work_sentence=work_sentence.replace(characters,characters+''+self._delimiter_token)\n",
        "        self.sentences= [x for x in work_sentence.split(self._delimiter_token) if x!='']\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._index<len(self.sentences):\n",
        "            result= self.sentences[self._index]\n",
        "            self._index+=1\n",
        "            return result\n",
        "        else:\n",
        "            raise StopIteration"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm9Wrftm9i87"
      },
      "source": [
        "# to tokenize a sentence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYbAOw5cAOEa"
      },
      "source": [
        "class Tokenizer():\n",
        "    import string\n",
        "    \n",
        "    def __init__(self, sentence , token_boundries=[' ','-','.'],delimiter_token='<SPLIT>',punctuations = string.punctuation ):\n",
        "        self.words = []\n",
        "        self.sentence = sentence.lower()\n",
        "        self._token_boundries = token_boundries\n",
        "        self._index = 0\n",
        "        self._punctuations = punctuations\n",
        "        self._delimiter_token = delimiter_token\n",
        "        self._tokenize()\n",
        "    def _tokenize(self):\n",
        "        work_sentence = self.sentence\n",
        "        token = self._delimiter_token\n",
        "        for characters in self._punctuations:\n",
        "            work_sentence = work_sentence.replace(characters , ''+ characters + \"\")\n",
        "        for characters in self._token_boundries:\n",
        "            work_sentence = work_sentence.replace(characters,token)\n",
        "        self.words= [x for x in work_sentence.split(token) if x!='']\n",
        "    def __iter__(self):\n",
        "        return self \n",
        "    def __next__(self):\n",
        "        if self._index<len(self.words):\n",
        "            result= self.words[self._index]\n",
        "            self._index+=1\n",
        "            return result \n",
        "        else:\n",
        "            self._index=0\n",
        "            raise StopIteration"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKyt7jVn9oXK"
      },
      "source": [
        "# Probable replacing words for the incorrect word in a sentence "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gigdjqQcf5gv"
      },
      "source": [
        "class Edit():\n",
        "    def __init__(self, words, alphabet='abcdefghijklmnopqrstuvwxyz'):\n",
        "        self.swords=[]\n",
        "        self.word = words \n",
        "        self. alphabet = alphabet \n",
        "        self.splits()\n",
        "        self.replace()\n",
        "        self.delete()\n",
        "        self.switch()\n",
        "        self.insert()\n",
        "        self.twords()\n",
        "        # self.replace()\n",
        "    def splits(self):\n",
        "        word = self.word\n",
        "        self.split = [(word[0:i],word[i:]) for i in range(len(word)+1)]\n",
        "    def replace(self):\n",
        "        wrong_word = self.word\n",
        "        splits = self.split\n",
        "        alphabet=self.alphabet\n",
        "        self.rwords = list(set([L +alphabet[j]+ K[1:] for L, K in self.split for j in range(len(alphabet))]))\n",
        "    def delete(self):\n",
        "        wrong_word = self.word\n",
        "        splits = self.split\n",
        "        self.dwords = list(set([L + K[1:] for L, K in self.split if K!=0]))\n",
        "    def switch(self):\n",
        "        wrong_word = self.word\n",
        "        splits = self.split\n",
        "        self.swords = list(set([L+R[1]+R[0]+R[2:] for L,R in splits if len(R)>=2]))\n",
        "    def insert(self):\n",
        "        wrong_word = self.word\n",
        "        splits = self.split\n",
        "        alphabet=self.alphabet\n",
        "        self.iwords = list(set([L+alphabet[j]+R for L, R in splits for j in range (len(alphabet))]))\n",
        "    def twords(self):\n",
        "        self.total_words= list(set(self.rwords+self.dwords+self.swords+self.iwords))    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3blogyTSYlL"
      },
      "source": [
        "''' main is the vocab containing the words and the number they have occured in a vocab so that  \n",
        "we know the word with their probability though this approch  seems less obvious, for better accurate \n",
        "we can also evaluate the probabaility of the simultaneous occuring of the word''' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0auvH2gw-UP8"
      },
      "source": [
        "#Generating the vocab from the given text file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpxEMQopIEUa"
      },
      "source": [
        "# generator for reading only word per step for large vocab\n",
        "def words_generator(file_name):\n",
        "    for line in open(file_name):\n",
        "        for word in re.findall('\\w+',line):\n",
        "            yield word.lower()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpIwsaI5IiTK"
      },
      "source": [
        "file_name = '/content/shakespere.txt'\n",
        "vocab = {}\n",
        "count = 0\n",
        "for word in (words_generator(file_name)):\n",
        "    vocab[word] = vocab.get(word,0)+1\n",
        "    count+=1\n",
        "vocab = { k: v/count for k, v in vocab.items()}#vocab with probabilities as values"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai48I04Z-g3k"
      },
      "source": [
        "# Determing all the possible replacement word for the incorrect word "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKqi-UmgsvUH"
      },
      "source": [
        "# n_corrections for allowing correction of amount of words in the incorrect word\n",
        "def correct_words(word, n_corrections=1):\n",
        "    probable_words = Edit(word)\n",
        "    total_correct_words = probable_words.total_words\n",
        "    for i in range(n_corrections-1):\n",
        "        for words in probable_words.total_words:\n",
        "            probab = Edit(words)\n",
        "            total_correct_words= total_correct_words+probab.total_words\n",
        "            #need to check if the word after replace,switch etc exist in vocab as only those in vocab\n",
        "            # are the possible solutions\n",
        "    return ([word for word in set(total_correct_words) if word in vocab.keys()])\n",
        "            \n",
        "    "
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oycHJF0QuSN8",
        "outputId": "6f174b1d-0315-4f04-859e-93daeb7f89d2"
      },
      "source": [
        "correct_word = correct_words('whre',n_corrections=2)\n",
        "print(correct_word[:10])\n",
        "correct_word = correct_words('whre',n_corrections=1)\n",
        "print(correct_word[:10])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['she', 'fore', 'wife', 'fare', 'white', 'whores', 'who', 'work', 'where', 'bore']\n",
            "['whoe', 'where', 'wore', 'were']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBh_TuFV__Ai"
      },
      "source": [
        "# Bringing it all together\n",
        "In this method we will use simple probabilistic method for single sentence.    \n",
        "we could build a probability of two words coming together to determine the word.    \n",
        "we could also use minmum distance for the correction which would be done later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FNkn472_9Gl",
        "outputId": "290c5421-5cfe-41d4-8424-3f507f2ceec0"
      },
      "source": [
        "def best_probability(words):\n",
        "    prbab = np.nan\n",
        "    for word in words:\n",
        "        if vocab[word] > prbab :\n",
        "            probab = vocab[word]\n",
        "            probab_words = word \n",
        "    return word\n",
        "\n",
        "def Autocorrect(sentence):\n",
        "    words = Tokenizer(sentence)\n",
        "    incorrect_words = [word for word in words if word not in vocab.keys()]\n",
        "    all_Words = {word:correct_words(word) for word in incorrect_words}\n",
        "    corrected_words = { i : best_probability(j) for i,j in all_Words.items()}\n",
        "    return corrected_words\n",
        "Autocorrect('This is wher it al starts')"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "where 0.0017532733987391353\n",
            "when 0.003674413399485209\n",
            "her 0.005259820196217406\n",
            "ay 0.0005035998060208155\n",
            "a 0.01411944641325027\n",
            "all 0.004812175924198903\n",
            "ail 1.865184466743761e-05\n",
            "ah 0.00014921475733950087\n",
            "am 0.002331480583429701\n",
            "tal 1.865184466743761e-05\n",
            "an 0.0017719252434065728\n",
            "ll 0.0016600141754019473\n",
            "at 0.0024433916514343267\n",
            "alt 1.865184466743761e-05\n",
            "as 0.0058939829149102846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'al': 'as', 'wher': 'her'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVol9HoRHxYR"
      },
      "source": [
        "As you can see this is really bad, i mean really really bad so we will try minimum distance method in near future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "273qsHLuHwik"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-otmlm2FJj2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}